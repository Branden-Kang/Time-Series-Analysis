{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multivariate MultiStep LSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP4kfvtcrHHOJtPWn2KOSo5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://bobrupakroy.medium.com/multivariate-multistep-lstm-38d9536a6b2e)"
      ],
      "metadata": {
        "id": "y0ufLqMR11Ma"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qOCaRBS1z17",
        "outputId": "d3b05973-8940-45d4-e4ce-12290d3bd2cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "#Lstm Multivariate Multi-Step\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from pandas import DataFrame , concat\n",
        "from sklearn.metrics import mean_absolute_error , mean_squared_error\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "from numpy import mean , concatenate\n",
        "from math import sqrt\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM,Activation\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "#from keras.layers import LSTM\n",
        "from numpy import array , hstack\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "dataset = pd.read_csv(\"https://raw.githubusercontent.com/Branden-Kang/Time-Series-Analysis/master/Data/LSTM-Multivariate_pollution.csv\", header=0, index_col=0)\n",
        "t = dataset.columns.tolist()\n",
        "dataset = dataset[['dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain','pollution']]\n",
        "#else slice is invalid for use in labelEncoder\n",
        "dataset= dataset.values\n",
        "# integer encode direction\n",
        "encoder = LabelEncoder()\n",
        "dataset[:,3] = encoder.fit_transform(dataset[:,3])\n",
        "#conver to pd.Dataframe else slices error\n",
        "dataset = pd.DataFrame(dataset)\n",
        "dataset.columns = ['dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain','pollution']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset[['dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain','pollution']]\n",
        "#Data Pre-processing step--------------------------------\n",
        "x_1 = dataset['dew'].values\n",
        "x_2 = dataset['temp'].values\n",
        "x_3 = dataset['press'].values\n",
        "x_4 = dataset['wnd_spd'].values\n",
        "x_5 = dataset['wnd_dir'].values\n",
        "x_6 = dataset['snow'].values\n",
        "x_7 = dataset['rain'].values\n",
        "y = dataset['pollution'].values\n",
        "#x_1 = x_1.values\n",
        "#x_2 = x_2.values\n",
        "#y = y.values\n",
        "# Step 1 : convert to [rows, columns] structure\n",
        "x_1 = x_1.reshape((len(x_1), 1))\n",
        "x_2 = x_2.reshape((len(x_2), 1))\n",
        "x_3 = x_3.reshape((len(x_3), 1))\n",
        "x_4 = x_4.reshape((len(x_4), 1))\n",
        "x_5 = x_5.reshape((len(x_5), 1))\n",
        "x_6 = x_6.reshape((len(x_6), 1))\n",
        "x_7 = x_7.reshape((len(x_7), 1))\n",
        "y = y.reshape((len(y), 1))\n",
        "print (\"x_1.shape\" , x_1.shape) \n",
        "print (\"x_2.shape\" , x_2.shape) \n",
        "print (\"y.shape\" , y.shape)\n",
        "# Step 2 : normalization \n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "x_1_scaled = scaler.fit_transform(x_1)\n",
        "x_2_scaled = scaler.fit_transform(x_2)\n",
        "x_3_scaled = scaler.fit_transform(x_3)\n",
        "x_4_scaled = scaler.fit_transform(x_4)\n",
        "x_5_scaled = scaler.fit_transform(x_5)\n",
        "x_6_scaled = scaler.fit_transform(x_6)\n",
        "x_7_scaled = scaler.fit_transform(x_7)\n",
        "y_scaled = scaler.fit_transform(y)\n",
        "# Step 3 : horizontally stack columns\n",
        "dataset_stacked = hstack((x_1_scaled, x_2_scaled,x_2_scaled, x_3_scaled,\n",
        "                          x_4_scaled, x_5_scaled,x_7_scaled, y_scaled))\n",
        "print (\"dataset_stacked.shape\" , dataset_stacked.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c75VMkjo27qo",
        "outputId": "16cae7f0-82da-47ea-e761-f58fff9e1765"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_1.shape (43800, 1)\n",
            "x_2.shape (43800, 1)\n",
            "y.shape (43800, 1)\n",
            "dataset_stacked.shape (43800, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. n_steps_in : Specify how much data we want to look back for prediction\n",
        "#2. n_step_out : Specify how much multi-step data we want to forecast\n",
        "# split a multivariate sequence into samples\n",
        "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
        " X, y = list(), list()\n",
        " for i in range(len(sequences)):\n",
        "  # find the end of this pattern\n",
        "  end_ix = i + n_steps_in\n",
        "  out_end_ix = end_ix + n_steps_out-1\n",
        "  # check if we are beyond the dataset\n",
        "  if out_end_ix > len(sequences):\n",
        "   break\n",
        "  # gather input and output parts of the pattern\n",
        "  seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
        "  X.append(seq_x)\n",
        "  y.append(seq_y)\n",
        " return array(X), array(y)\n",
        "# choose a number of time steps #change this accordingly\n",
        "n_steps_in, n_steps_out = 60 , 30\n",
        "# covert into input/output\n",
        "X, y = split_sequences(dataset_stacked, n_steps_in, n_steps_out)\n",
        "print (\"X.shape\" , X.shape) \n",
        "print (\"y.shape\" , y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOemi-vF2_js",
        "outputId": "3dbe2348-dd09-4b1b-a161-606ade7e4371"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape (43712, 60, 7)\n",
            "y.shape (43712, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_X, test_X,train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "#split_point = 1258*25\n",
        "#train_X , train_y = X[:split_point, :] , y[:split_point, :]\n",
        "#test_X , test_y = X[split_point:, :] , y[split_point:, :]\n",
        "train_X.shape #[n_datasets,n_steps_in,n_features]\n",
        "train_y.shape #[n_datasets,n_steps_out]\n",
        "test_X.shape \n",
        "test_y.shape \n",
        "n_features = 7\n",
        "#number of features\n",
        "#n_features = 2\n",
        "#optimizer learning rate\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "model.add(Dense(n_steps_out))\n",
        "model.add(Activation('linear'))\n",
        "model.compile(loss='mse' , optimizer=opt , metrics=['accuracy'])\n",
        "# Fit network\n",
        "# history = model.fit(train_X , train_y , epochs=1500, steps_per_epoch=25 , verbose=1 ,validation_data=(test_X, test_y) ,shuffle=False)\n",
        "history = model.fit(train_X , train_y , epochs=10, steps_per_epoch=5 , verbose=1 ,validation_data=(test_X, test_y) ,shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDLhH2Km3DEE",
        "outputId": "2aaf3674-f983-46d6-f659-75090683325e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5/5 [==============================] - 38s 7s/step - loss: 0.0180 - accuracy: 0.0241 - val_loss: 0.0174 - val_accuracy: 0.0234\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 22s 5s/step - loss: 0.0172 - accuracy: 0.0271 - val_loss: 0.0166 - val_accuracy: 0.0248\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 22s 5s/step - loss: 0.0165 - accuracy: 0.0290 - val_loss: 0.0159 - val_accuracy: 0.0260\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 24s 5s/step - loss: 0.0158 - accuracy: 0.0297 - val_loss: 0.0153 - val_accuracy: 0.0261\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 23s 5s/step - loss: 0.0152 - accuracy: 0.0293 - val_loss: 0.0147 - val_accuracy: 0.0254\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 23s 5s/step - loss: 0.0146 - accuracy: 0.0293 - val_loss: 0.0142 - val_accuracy: 0.0263\n",
            "Epoch 7/10\n",
            "5/5 [==============================] - 21s 4s/step - loss: 0.0141 - accuracy: 0.0291 - val_loss: 0.0137 - val_accuracy: 0.0278\n",
            "Epoch 8/10\n",
            "5/5 [==============================] - 22s 5s/step - loss: 0.0136 - accuracy: 0.0275 - val_loss: 0.0132 - val_accuracy: 0.0284\n",
            "Epoch 9/10\n",
            "5/5 [==============================] - 21s 4s/step - loss: 0.0131 - accuracy: 0.0275 - val_loss: 0.0128 - val_accuracy: 0.0277\n",
            "Epoch 10/10\n",
            "5/5 [==============================] - 22s 5s/step - loss: 0.0127 - accuracy: 0.0277 - val_loss: 0.0124 - val_accuracy: 0.0280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST DATA----------------------------------------\n",
        "dataset_test_ok = pd.read_csv('pollution_test_data1.csv')\n",
        "dataset_test_ok.head()\n",
        "#pre-process the new data into the same input format provided during #training the lstm\n",
        "# integer encode direction\n",
        "encoder1 = LabelEncoder()\n",
        "dataset_test_ok.iloc[:,3] = encoder1.fit_transform(dataset_test_ok.iloc[:,3])\n",
        "# read test data\n",
        "x1_test = dataset_test_ok['dew'].values\n",
        "x2_test = dataset_test_ok['temp'].values\n",
        "x3_test = dataset_test_ok['press'].values\n",
        "x4_test = dataset_test_ok['wnd_spd'].values\n",
        "x5_test = dataset_test_ok['wnd_dir'].values\n",
        "x6_test = dataset_test_ok['snow'].values\n",
        "x7_test = dataset_test_ok['rain'].values\n",
        "y_test = dataset_test_ok['pollution'].values # no need to scale\n",
        "# convert to [rows, columns] structure\n",
        "x1_test = x1_test.reshape((len(x1_test), 1))\n",
        "x2_test = x2_test.reshape((len(x2_test), 1))\n",
        "x3_test = x3_test.reshape((len(x3_test), 1))\n",
        "x4_test = x4_test.reshape((len(x4_test), 1))\n",
        "x5_test = x5_test.reshape((len(x5_test), 1))\n",
        "x6_test = x6_test.reshape((len(x6_test), 1))\n",
        "x7_test = x7_test.reshape((len(x7_test), 1))\n",
        "y_test = y_test.reshape((len(y_test), 1))\n",
        "x1_test_scaled = scaler.fit_transform(x1_test)\n",
        "x2_test_scaled = scaler.fit_transform(x2_test)\n",
        "x3_test_scaled = scaler.fit_transform(x3_test)\n",
        "x4_test_scaled = scaler.fit_transform(x4_test)\n",
        "x5_test_scaled = scaler.fit_transform(x5_test)\n",
        "x6_test_scaled = scaler.fit_transform(x6_test)\n",
        "x7_test_scaled = scaler.fit_transform(x7_test)\n",
        "# Step 3 : horizontally stack columns\n",
        "dataset_test_stacked = hstack((x1_test_scaled,x2_test_scaled,x3_test_scaled,x4_test_scaled,\n",
        "                               x5_test_scaled,x6_test_scaled,x7_test_scaled))\n",
        "print (\"dataset_stacked.shape\" , dataset_test_stacked.shape)"
      ],
      "metadata": {
        "id": "RHMIHNq93awy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Prediction#######################################\n",
        "\n",
        "dataset_test_X = dataset_test_stacked\n",
        "print(\"dataset_test_X :\",dataset_test_X.shape)\n",
        "test_X_new = dataset_test_X.reshape(1,dataset_test_X.shape[0],dataset_test_X.shape[1])\n",
        "y_pred = model.predict(test_X_new)\n",
        "y_pred_inv = scaler1.inverse_transform(y_pred)\n",
        "y_pred_inv = y_pred_inv.reshape(n_steps_out,1)\n",
        "y_pred_inv = y_pred_inv[:,0]\n",
        "print(\"y_pred :\",y_pred.shape)\n",
        "print(\"y_pred_inv :\",y_pred_inv.shape)"
      ],
      "metadata": {
        "id": "UE4zLXiF3erc"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}