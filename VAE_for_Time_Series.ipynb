{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5PiOrHO+uDC/osyQaffmv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://towardsdatascience.com/vae-for-time-series-1dc0fef4bffa)"
      ],
      "metadata": {
        "id": "i0O7SbZ4rrDz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Uulg1zRtq2ac"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z\"\"\"\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        rank = len(z_mean.shape)\n",
        "\n",
        "        if rank == 2:  # 2D case\n",
        "            dim = tf.shape(z_mean)[1]\n",
        "            epsilon_shape = (batch, dim)\n",
        "        elif rank == 1:  # 1D case\n",
        "            epsilon_shape = (batch,)\n",
        "        elif rank == 3:  # 3D case\n",
        "            dim1 = tf.shape(z_mean)[1]\n",
        "            dim2 = tf.shape(z_mean)[2]\n",
        "            epsilon_shape = (batch, dim1, dim2)\n",
        "        else:\n",
        "            raise ValueError(\"z_mean and z_log_var must be 1D, 2D, or 3D tensors\")\n",
        "\n",
        "        epsilon = tf.keras.backend.random_normal(shape=epsilon_shape)\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "def kl_divergence_sum(mu1 = 0.0, log_var1 = 0.0, mu2 = 0.0, log_var2 = 0.0):\n",
        "    var1 = tf.exp(log_var1)\n",
        "    var2 = tf.exp(log_var2)\n",
        "    axis0 = 0.5*tf.reduce_mean(log_var2 - log_var1 + (var1 + (mu1 - mu2)**2) / var2 - 1, axis=0)\n",
        "    return tf.reduce_sum(axis0)\n",
        "\n",
        "\n",
        "\n",
        "def log_lik_normal_sum(x, mu=0.0, log_var = 0.0):\n",
        "    axis0 = -0.5*(tf.math.log(2*np.pi) + tf.reduce_mean(log_var + (x - mu) ** 2 * tf.exp(-log_var), axis=0))\n",
        "    return tf.reduce_sum(axis0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(None,)) # (N, 96*k)\n",
        "x = layers.Reshape((-1, 1))(inputs)  # (N, 96*k, 1)\n",
        "\n",
        "# Conv1D parameters: filters, kernel_size, strides, padding\n",
        "x = layers.Conv1D(40, 5, 3, 'same', activation='relu')(x) # (N, 32*k, 40)\n",
        "x = layers.Conv1D(40, 3, 2, 'same', activation='relu')(x) # (N, 16*k, 40)\n",
        "x = layers.Conv1D(40, 3, 2, 'same', activation='relu')(x) # (N, 8*k, 40)\n",
        "x = layers.Conv1D(40, 3, 2, 'same', activation='relu')(x) # (N, 4*k, 40)\n",
        "x = layers.Conv1D(40, 3, 2, 'same', activation='relu')(x) # (N, 2*k, 40)\n",
        "x = layers.Conv1D(20, 3, 2, 'same')(x) # (N, k, 20)\n",
        "\n",
        "z_mean = x[: ,:, :10]   # (N, k, 10)\n",
        "z_log_var = x[:, :, 10:] # (N, k, 10)\n",
        "z = Sampling()([z_mean, z_log_var]) # custom layer sampling from gaussian\n",
        "\n",
        "encoder = models.Model(inputs, [z_mean, z_log_var, z], name='encoder')"
      ],
      "metadata": {
        "id": "E9Lt9JDSrAaQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input shape: (batch_size, time_length/96, latent_features)\n",
        "inputs = layers.Input(shape=(None, 10)) # (N, k, 10)\n",
        "\n",
        "# Conv1DTranspose parameters: filters, kernel_size, strides, padding\n",
        "x = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(inputs) # (N, 2*k, 40)\n",
        "x = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(x) # (N, 4*k, 40)\n",
        "x = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(x) # (N, 8*k, 40)\n",
        "x = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(x) # (N, 16*k, 40)\n",
        "x = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(x) # (N, 32*k, 40)\n",
        "x = layers.Conv1DTranspose(1,  5, 3, 'same')(x) # (N, 96*k, 1)\n",
        "\n",
        "outputs = layers.Reshape((-1,))(x) # (N, 96*k)\n",
        "\n",
        "decoder = models.Model(inputs, outputs, name='decoder')"
      ],
      "metadata": {
        "id": "b2-ZNQzerEek"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(models.Model):\n",
        "    def __init__(self, encoder, decoder, prior, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.prior = prior\n",
        "        self.noise_log_var = self.add_weight(name='var', shape=(1,), initializer='zeros', trainable=True)\n",
        "\n",
        "    @tf.function\n",
        "    def vae_loss(self, data):\n",
        "        values, seasonal = data\n",
        "        z_mean, z_log_var, z = self.encoder(values)\n",
        "        reconstructed = self.decoder(z)\n",
        "        reconstruction_loss = -log_lik_normal_sum(values, reconstructed, self.noise_log_var)/INPUT_SIZE\n",
        "        seasonal_z_mean, seasonal_z_log_var, _ = self.prior(seasonal)\n",
        "        kl_loss_z = kl_divergence_sum(z_mean, z_log_var, seasonal_z_mean, seasonal_z_log_var)/INPUT_SIZE\n",
        "        return reconstruction_loss, kl_loss_z\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            reconstruction_loss, kl_loss_z = self.vae_loss(data)\n",
        "            total_loss = reconstruction_loss + kl_loss_z\n",
        "\n",
        "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        return {'loss': total_loss}"
      ],
      "metadata": {
        "id": "Y7bQ9seIriHM"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}