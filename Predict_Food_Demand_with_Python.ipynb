{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc+CF8V1zNf1VDgnge8hir"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/thedeephub/predict-food-demand-with-python-part-2-2ede9b36e9d0)"
      ],
      "metadata": {
        "id": "Vs88kxvwgIFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "!pip install category_encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcselpjpjdOS",
        "outputId": "754448aa-1a69-44e7-90a4-217f2b9f9890"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.5\n",
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.10/dist-packages (2.6.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.1.4)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import matplotlib.dates as mdates\n",
        "%matplotlib inline\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.stattools import kpss\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from category_encoders.binary import BinaryEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, ReLU, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "from sklearn.preprocessing import QuantileTransformer, PowerTransformer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Mounting Google Drive\n",
        "from google.colab import drive  # Import for accessing Google Drive\n",
        "\n",
        "# Unzipping files\n",
        "import zipfile  # Import for extracting zip files"
      ],
      "metadata": {
        "id": "mgIPODikjYrC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read data"
      ],
      "metadata": {
        "id": "vGN_ltDNkGKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Get the file path from Google Drive\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/datasets/Forecasting_of_Food_Demand.zip'\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    # Extract the CSV files\n",
        "    train_path = zip_ref.extract('train.csv', '/content/')\n",
        "    test_path = zip_ref.extract('test.csv', '/content/')\n",
        "    sub_path = zip_ref.extract('sample_submission.csv', '/content/')\n",
        "    meals_path = zip_ref.extract('meal_info.csv', '/content/')\n",
        "    centres_path = zip_ref.extract('fulfilment_center_info.csv', '/content/')\n",
        "\n",
        "# Read the CSV files\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "sub = pd.read_csv(sub_path)\n",
        "meals = pd.read_csv(meals_path)\n",
        "centres = pd.read_csv(centres_path)"
      ],
      "metadata": {
        "id": "jiH6IFWlkFE2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features Scaling and Encoding"
      ],
      "metadata": {
        "id": "MU3efeCqh9WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_week = train[['week']]\n",
        "\n",
        "categoric_columns = ['center_id', 'meal_id', 'emailer_for_promotion', 'homepage_featured', 'city_code', 'region_code', 'center_type', 'op_area', 'category', 'cuisine']\n",
        "columns = list(train.columns)\n",
        "numeric_columns = [i for i in columns if i not in categoric_columns]\n",
        "\n",
        "numeric_columns.remove('num_orders')"
      ],
      "metadata": {
        "id": "8lI7HFa9h3J9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = BinaryEncoder(drop_invariant=False, return_df=True,)\n",
        "encoder.fit(train[categoric_columns])"
      ],
      "metadata": {
        "id": "DI3PNuWkiD8Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.set_output(transform=\"pandas\")\n",
        "quantile_transformer = QuantileTransformer(output_distribution='normal')\n",
        "train_num_quantile = quantile_transformer.fit_transform(train[numeric_columns])\n",
        "scaler.fit(train_num_quantile)"
      ],
      "metadata": {
        "id": "vwmNzWvciG6v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_cat = encoder.transform(train[categoric_columns])\n",
        "\n",
        "scaled_num = scaler.transform(train[numeric_columns])\n",
        "# encoded_cat = train[categoric_columns].apply(encoder.fit_transform)\n",
        "\n",
        "train = pd.concat([scaled_num, encoded_cat, train.num_orders], axis=1)"
      ],
      "metadata": {
        "id": "ccdl8WqZiIjj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['week_unscaled'] = train_week\n",
        "# Split the dataset into training (weeks 1-135) and evaluation (weeks 136-145) sets\n",
        "trainn = train[train['week_unscaled'] <= 135]\n",
        "evall = train[train['week_unscaled'] > 135]\n",
        "\n",
        "# Display the shapes of the training and evaluation sets\n",
        "print(\"Training set shape:\", trainn.shape)\n",
        "print(\"Evaluation set shape:\", evall.shape)\n",
        "\n",
        "trainn.drop('week_unscaled', axis=1, inplace=True)\n",
        "evall.drop('week_unscaled', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "djZVGmfPiKbU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into parts\n",
        "X_train = trainn.drop(['num_orders'], axis = 1)\n",
        "X_test = evall.drop(['num_orders'], axis = 1)\n",
        "y_train = trainn['num_orders']\n",
        "y_test = evall['num_orders']"
      ],
      "metadata": {
        "id": "Ed6yBMjJkipD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Models for Forecasting"
      ],
      "metadata": {
        "id": "kZS187VsiPB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [8, 9, 10],\n",
        "    'max_features': ['sqrt'],\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'min_samples_leaf': [2, 3, 4]\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "forest = RandomForestRegressor()\n",
        "model_forest = forest.fit(X_train, y_train)\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=forest, param_grid=param_grid, scoring='neg_mean_squared_log_error', cv=5)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best RMSLE score\n",
        "best_params = grid_search.best_params_\n",
        "best_rmsle = np.sqrt(-grid_search.best_score_)\n",
        "\n",
        "# Print the best parameters and best RMSLE score\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best RMSLE Score:\", best_rmsle)"
      ],
      "metadata": {
        "id": "cob4HX3piNlR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [8, 9, 10],\n",
        "    'max_features': ['sqrt'],\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'min_samples_leaf': [2, 3, 4]\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "forest = RandomForestRegressor()\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=forest, param_grid=param_grid, scoring='neg_mean_squared_log_error', cv=5)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best RMSLE score\n",
        "best_params = grid_search.best_params_\n",
        "best_rmsle = np.sqrt(-grid_search.best_score_)\n",
        "\n",
        "# Print the best parameters and best RMSLE score\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best RMSLE Score:\", best_rmsle)"
      ],
      "metadata": {
        "id": "ZGids5B5i-RL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting Machine\n",
        "# Initialize and fit the Gradient Boosting model\n",
        "gbr = GradientBoostingRegressor()\n",
        "model_gbr = gbr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on X_test\n",
        "gbr_pred = model_gbr.predict(X_test)\n",
        "\n",
        "# Light GBM\n",
        "# Initialize and fit the LightGBM Regressor\n",
        "lgbm = lgb.LGBMRegressor()\n",
        "model_lgbm = lgbm.fit(X_train, y_train)\n",
        "\n",
        "# XGBoost\n",
        "# Initialize and fit the XGBoost model\n",
        "xgboost = xgb.XGBRegressor()\n",
        "model_xgboost = xgboost.fit(X_train, y_train)\n",
        "\n",
        "# CatBoost Regressor\n",
        "# Initialize and fit the CatBoost Regressor\n",
        "catboost = cb.CatBoostRegressor(silent=True)\n",
        "model_catboost = catboost.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "elJsn4s_ktBO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_pred = model_forest.predict(X_test)\n",
        "mse = mean_squared_error(y_test, forest_pred)\n",
        "msle = mean_squared_log_error(y_test, forest_pred)\n",
        "rmse = np.sqrt(mse).round(2)\n",
        "rmsle = np.sqrt(msle).round(5)\n",
        "\n",
        "# Append the results to the DataFrame\n",
        "results = pd.DataFrame([['Random Forest', mse, msle, rmse, rmsle]],\n",
        "                             columns=['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])\n",
        "\n",
        "gbr_pred = model_gbr.predict(X_test)\n",
        "gbr_pred = np.abs(gbr_pred)\n",
        "# Append the results to the DataFrame\n",
        "mse = mean_squared_error(y_test, gbr_pred)\n",
        "msle = mean_squared_log_error(y_test, gbr_pred)\n",
        "rmse = np.sqrt(mse).round(2)\n",
        "rmsle = np.sqrt(msle).round(5)\n",
        "\n",
        "model_results = pd.DataFrame([['Gradient Boosting', mse, msle, rmse, rmsle]],\n",
        "                             columns=['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])\n",
        "results = pd.concat([results, model_results], ignore_index=True)\n",
        "\n",
        "lgbm_pred = np.abs(model_lgbm.predict(X_test))\n",
        "\n",
        "# Compute performance metrics\n",
        "mse = mean_squared_error(y_test, lgbm_pred)\n",
        "msle = mean_squared_log_error(y_test, lgbm_pred)\n",
        "rmse = np.sqrt(mse).round(2)\n",
        "rmsle = np.sqrt(msle).round(5)\n",
        "\n",
        "# Create a DataFrame for the model results\n",
        "model_results = pd.DataFrame([['LightGBM', mse, msle, rmse, rmsle]],\n",
        "                             columns=['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])\n",
        "\n",
        "# Concatenate the new results to the existing results DataFrame\n",
        "results = pd.concat([results, model_results], ignore_index=True)\n",
        "\n",
        "xgboost_pred = np.abs(model_xgboost.predict(X_test))\n",
        "\n",
        "# Append the results to the DataFrame\n",
        "mse = mean_squared_error(y_test, xgboost_pred)\n",
        "msle = mean_squared_log_error(y_test, xgboost_pred)\n",
        "rmse = np.sqrt(mse).round(2)\n",
        "rmsle = np.sqrt(msle).round(5)\n",
        "\n",
        "model_results = pd.DataFrame([['XGBoost', mse, msle, rmse, rmsle]],\n",
        "                             columns=['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])\n",
        "results = pd.concat([results, model_results], ignore_index=True)\n",
        "\n",
        "catboost_pred = np.abs(model_catboost.predict(X_test))\n",
        "\n",
        "# Compute performance metrics\n",
        "mse = mean_squared_error(y_test, catboost_pred)\n",
        "msle = mean_squared_log_error(y_test, catboost_pred)\n",
        "rmse = np.sqrt(mse).round(2)\n",
        "rmsle = np.sqrt(msle).round(5)\n",
        "\n",
        "# Create a DataFrame for the model results\n",
        "model_results = pd.DataFrame([['CatBoost', mse, msle, rmse, rmsle]],\n",
        "                             columns=['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])\n",
        "\n",
        "# Concatenate the new results to the existing results DataFrame\n",
        "results = pd.concat([results, model_results], ignore_index=True)\n",
        "results"
      ],
      "metadata": {
        "id": "-pbuFhwnjAWy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequences for LSTM input\n",
        "def create_sequences(X, y, time_steps=10):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X.iloc[i:(i + time_steps)].values)\n",
        "        ys.append(y.iloc[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "time_steps = 10\n",
        "X_train_seq, y_train_seq = create_sequences(X_train, y_train, time_steps)\n",
        "X_test_seq, y_test_seq = create_sequences(X_test, y_test, time_steps)\n",
        "\n",
        "# Reshape y_train_seq and y_test_seq to be 2D arrays\n",
        "y_train_seq = y_train_seq.reshape(-1, 1)\n",
        "y_test_seq = y_test_seq.reshape(-1, 1)\n",
        "\n",
        "# Check the shapes\n",
        "print(X_train_seq.shape, y_train_seq.shape)\n",
        "print(X_test_seq.shape, y_test_seq.shape)\n",
        "\n",
        "# Now proceed to create and train the LSTM model\n",
        "\n",
        "# Define the LSTM model based on the provided architecture\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM layer 1\n",
        "    model.add(LSTM(64, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(ReLU())\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    # LSTM layer 2\n",
        "    model.add(LSTM(32, return_sequences=True))\n",
        "    model.add(ReLU())\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    # LSTM layer 3\n",
        "    model.add(LSTM(16))\n",
        "    model.add(ReLU())\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    # Dense layer\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the Bi-LSTM model\n",
        "def create_bilstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Bi-LSTM layer 1\n",
        "    model.add(Bidirectional(LSTM(32, return_sequences=True, dropout=0.25, recurrent_activation='tanh'), input_shape=input_shape))\n",
        "\n",
        "    # Bi-LSTM layer 2\n",
        "    model.add(Bidirectional(LSTM(16, return_sequences=False, dropout=0.25, recurrent_activation='tanh')))\n",
        "\n",
        "    # Dense layer\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "wZ_9Ua_ejEYd"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}