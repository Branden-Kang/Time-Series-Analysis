{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdc6/06Cei+yxxPrFhJaUI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@palashm0002/predicting-time-series-data-with-machine-learning-generative-ai-and-deep-learning-36bf99ad6f5e)"
      ],
      "metadata": {
        "id": "X-VpCys_vUyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NVjfx7DdvR76"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# Load your time series data\n",
        "time_series_data = pd.read_csv('time_series_data.csv')\n",
        "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
        "time_series_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Fit ARIMA model\n",
        "model = ARIMA(time_series_data['Value'], order=(5, 1, 0))  # (p,d,q)\n",
        "model_fit = model.fit()\n",
        "\n",
        "# Make predictions\n",
        "predictions = model_fit.forecast(steps=10)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# Load your time series data\n",
        "time_series_data = pd.read_csv('time_series_data.csv')\n",
        "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
        "time_series_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Fit SARIMA model\n",
        "model = SARIMAX(time_series_data['Value'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))  # (p,d,q) (P,D,Q,s)\n",
        "model_fit = model.fit(disp=False)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model_fit.forecast(steps=10)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "yOd92vymvYij"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet\n",
        "import pandas as pd\n",
        "\n",
        "# Load your time series data\n",
        "time_series_data = pd.read_csv('time_series_data.csv')\n",
        "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
        "time_series_data.rename(columns={'Date': 'ds', 'Value': 'y'}, inplace=True)\n",
        "\n",
        "# Fit Prophet model\n",
        "model = Prophet()\n",
        "model.fit(time_series_data)\n",
        "\n",
        "# Make future dataframe and predictions\n",
        "future = model.make_future_dataframe(periods=10)\n",
        "forecast = model.predict(future)\n",
        "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']])"
      ],
      "metadata": {
        "id": "G_Fa619gvZUt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load your time series data\n",
        "time_series_data = pd.read_csv('time_series_data.csv')\n",
        "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
        "time_series_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Prepare data for supervised learning\n",
        "def create_lag_features(data, lag=1):\n",
        "    df = data.copy()\n",
        "    for i in range(1, lag + 1):\n",
        "        df[f'lag_{i}'] = df['Value'].shift(i)\n",
        "    return df.dropna()\n",
        "\n",
        "lag = 5\n",
        "data_with_lags = create_lag_features(time_series_data, lag=lag)\n",
        "X = data_with_lags.drop('Value', axis=1)\n",
        "y = data_with_lags['Value']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Fit XGBoost model\n",
        "model = XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')"
      ],
      "metadata": {
        "id": "KgkWDfU0vbjj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, LeakyReLU, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load your time series data\n",
        "time_series_data = pd.read_csv('time_series_data.csv')\n",
        "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
        "time_series_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Prepare data for GAN\n",
        "def create_dataset(dataset, time_step=1):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(dataset)-time_step-1):\n",
        "        a = dataset[i:(i+time_step), 0]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[i + time_step, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "time_step = 10\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
        "\n",
        "X_train, y_train = create_dataset(scaled_data, time_step)\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "\n",
        "# GAN components\n",
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=time_step))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(time_step, activation='tanh'))\n",
        "    model.add(Reshape((time_step, 1)))\n",
        "    return model\n",
        "\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, input_shape=(time_step, 1)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Build and compile the discriminator\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "# Build the generator\n",
        "generator = build_generator()\n",
        "\n",
        "# The generator takes noise as input and generates data\n",
        "z = Input(shape=(time_step,))\n",
        "generated_data = generator(z)\n",
        "\n",
        "# For the combined model, we will only train the generator\n",
        "discriminator.trainable = False\n",
        "\n",
        "# The discriminator takes generated data as input and determines validity\n",
        "validity = discriminator(generated_data)\n",
        "\n",
        "# The combined model (stacked generator and discriminator)\n",
        "combined = Model(z, validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# Training the GAN\n",
        "epochs = 10000\n",
        "batch_size = 32\n",
        "valid = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # ---------------------\n",
        "    #  Train Discriminator\n",
        "    # ---------------------\n",
        "\n",
        "    # Select a random batch of real data\n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    real_data = X_train[idx]\n",
        "\n",
        "    # Generate a batch of fake data\n",
        "    noise = np.random.normal(0, 1, (batch_size, time_step))\n",
        "    gen_data = generator.predict(noise)\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(real_data, valid)\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_data, fake)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # ---------------------\n",
        "    #  Train Generator\n",
        "    # ---------------------\n",
        "\n",
        "    noise = np.random.normal(0, 1, (batch_size, time_step))\n",
        "\n",
        "    # Train the generator (to have the discriminator label samples as valid)\n",
        "    g_loss = combined.train_on_batch(noise, valid)\n",
        "\n",
        "    # Print the progress\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
        "\n",
        "# Make predictions\n",
        "noise = np.random.normal(0, 1, (1, time_step))\n",
        "generated_prediction = generator.predict(noise)\n",
        "generated_prediction = scaler.inverse_transform(generated_prediction)\n",
        "print(generated_prediction)"
      ],
      "metadata": {
        "id": "7X46omvlvdwE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, LeakyReLU, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load your time series data\n",
        "time_series_data = pd.read_csv('time_series_data.csv')\n",
        "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
        "time_series_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Prepare data for GAN\n",
        "def create_dataset(dataset, time_step=1):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(dataset)-time_step-1):\n",
        "        a = dataset[i:(i+time_step), 0]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[i + time_step, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "time_step = 10\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
        "\n",
        "X_train, y_train = create_dataset(scaled_data, time_step)\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "\n",
        "# GAN components\n",
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=time_step))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(time_step, activation='tanh'))\n",
        "    model.add(Reshape((time_step, 1)))\n",
        "    return model\n",
        "\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, input_shape=(time_step, 1)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Build and compile the discriminator\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "# Build the generator\n",
        "generator = build_generator()\n",
        "\n",
        "# The generator takes noise as input and generates data\n",
        "z = Input(shape=(time_step,))\n",
        "generated_data = generator(z)\n",
        "\n",
        "# For the combined model, we will only train the generator\n",
        "discriminator.trainable = False\n",
        "\n",
        "# The discriminator takes generated data as input and determines validity\n",
        "validity = discriminator(generated_data)\n",
        "\n",
        "# The combined model (stacked generator and discriminator)\n",
        "combined = Model(z, validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# Training the GAN\n",
        "epochs = 10000\n",
        "batch_size = 32\n",
        "valid = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # ---------------------\n",
        "    #  Train Discriminator\n",
        "    # ---------------------\n",
        "\n",
        "    # Select a random batch of real data\n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    real_data = X_train[idx]\n",
        "\n",
        "    # Generate a batch of fake data\n",
        "    noise = np.random.normal(0, 1, (batch_size, time_step))\n",
        "    gen_data = generator.predict(noise)\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(real_data, valid)\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_data, fake)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # ---------------------\n",
        "    #  Train Generator\n",
        "    # ---------------------\n",
        "\n",
        "    noise = np.random.normal(0, 1, (batch_size, time_step))\n",
        "\n",
        "    # Train the generator (to have the discriminator label samples as valid)\n",
        "    g_loss = combined.train_on_batch(noise, valid)\n",
        "\n",
        "    # Print the progress\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
        "\n",
        "# Make predictions\n",
        "noise = np.random.normal(0, 1, (1, time_step))\n",
        "generated_prediction = generator.predict(noise)\n",
        "generated_prediction = scaler.inverse_transform(generated_prediction)\n",
        "print(generated_prediction)"
      ],
      "metadata": {
        "id": "jML5mqfDvgoe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load your time series data\n",
        "time_series_data = pd.read_csv('time_series_data.csv')\n",
        "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
        "time_series_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Prepare data for LSTM\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
        "\n",
        "train_size = int(len(scaled_data) * 0.8)\n",
        "train_data = scaled_data[:train_size]\n",
        "test_data = scaled_data[train_size:]\n",
        "\n",
        "def create_dataset(dataset, time_step=1):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(dataset)-time_step-1):\n",
        "        a = dataset[i:(i+time_step), 0]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[i + time_step, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "time_step = 10\n",
        "X_train, y_train = create_dataset(train_data, time_step)\n",
        "X_test, y_test = create_dataset(test_data, time_step)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
        "model.add(LSTM(50, return_sequences=False))\n",
        "model.add(Dense(25))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(X_train, y_train, batch_size=1, epochs=1)\n",
        "\n",
        "# Make predictions\n",
        "train_predict = model.predict(X_train)\n",
        "test_predict = model.predict(X_test)\n",
        "\n",
        "train_predict = scaler.inverse_transform(train_predict)\n",
        "test_predict = scaler.inverse_transform(test_predict)\n",
        "print(test_predict)"
      ],
      "metadata": {
        "id": "DacUFUW0vkWn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load your time series data\n",
        "time_series_data = pd.read_csv('time_series_data.csv')\n",
        "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
        "time_series_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Prepare data for GRU\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
        "\n",
        "train_size = int(len(scaled_data) * 0.8)\n",
        "train_data = scaled_data[:train_size]\n",
        "test_data = scaled_data[train_size:]\n",
        "\n",
        "def create_dataset(dataset, time_step=1):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(dataset)-time_step-1):\n",
        "        a = dataset[i:(i+time_step), 0]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[i + time_step, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "time_step = 10\n",
        "X_train, y_train = create_dataset(train_data, time_step)\n",
        "X_test, y_test = create_dataset(test_data, time_step)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Build GRU model\n",
        "model = Sequential()\n",
        "model.add(GRU(50, return_sequences=True, input_shape=(time_step, 1)))\n",
        "model.add(GRU(50, return_sequences=False))\n",
        "model.add(Dense(25))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(X_train, y_train, batch_size=1, epochs=1)\n",
        "\n",
        "# Make predictions\n",
        "train_predict = model.predict(X_train)\n",
        "test_predict = model.predict(X_test)\n",
        "\n",
        "train_predict = scaler.inverse_transform(train_predict)\n",
        "test_predict = scaler.inverse_transform(test_predict)\n",
        "print(test_predict)"
      ],
      "metadata": {
        "id": "gKduw-vZvnNK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, MultiHeadAttention, LayerNormalization, Dropout\n",
        "\n",
        "# Load your time series data\n",
        "time_series_data = pd.read_csv('time_series_data.csv')\n",
        "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
        "time_series_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Prepare data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
        "\n",
        "train_size = int(len(scaled_data) * 0.8)\n",
        "train_data = scaled_data[:train_size]\n",
        "test_data = scaled_data[train_size:]\n",
        "\n",
        "def create_dataset(dataset, time_step=1):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(dataset)-time_step-1):\n",
        "        a = dataset[i:(i+time_step), 0]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[i + time_step, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "time_step = 10\n",
        "X_train, y_train = create_dataset(train_data, time_step)\n",
        "X_test, y_test = create_dataset(test_data, time_step)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Build Transformer model\n",
        "model = Sequential()\n",
        "model.add(MultiHeadAttention(num_heads=4, key_dim=2, input_shape=(time_step, 1)))\n",
        "model.add(LayerNormalization())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(X_train, y_train, batch_size=1, epochs=1)\n",
        "\n",
        "# Make predictions\n",
        "train_predict = model.predict(X_train)\n",
        "test_predict = model.predict(X_test)\n",
        "\n",
        "train_predict = scaler.inverse_transform(train_predict)\n",
        "test_predict = scaler.inverse_transform(test_predict)\n",
        "print(test_predict)"
      ],
      "metadata": {
        "id": "MpiFdttpvpv4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "\n",
        "# Load your time series data\n",
        "time_series_data = pd.read_csv('time_series_data.csv')\n",
        "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
        "time_series_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Prepare data for Seq2Seq\n",
        "def create_dataset(dataset, time_step=1):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(dataset)-time_step-1):\n",
        "        a = dataset[i:(i+time_step), 0]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[i + time_step, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "time_step = 10\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
        "\n",
        "X, y = create_dataset(scaled_data, time_step)\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Define Seq2Seq model\n",
        "encoder_inputs = Input(shape=(time_step, 1))\n",
        "encoder = LSTM(50, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "decoder_inputs = Input(shape=(time_step, 1))\n",
        "decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n",
        "decoder_dense = Dense(1)\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit([X, X], y, epochs=10, batch_size=16)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict([X, X])\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "myHYZGpgvtYU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dense, Flatten\n",
        "\n",
        "# Load your time series data\n",
        "time_series_data = pd.read_csv('time_series_data.csv')\n",
        "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
        "time_series_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Prepare data for TCN\n",
        "def create_dataset(dataset, time_step=1):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(dataset)-time_step-1):\n",
        "        a = dataset[i:(i+time_step), 0]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[i + time_step, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "time_step = 10\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
        "\n",
        "X, y = create_dataset(scaled_data, time_step)\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Define TCN model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, dilation_rate=1, activation='relu', input_shape=(time_step, 1)))\n",
        "model.add(Conv1D(filters=64, kernel_size=2, dilation_rate=2, activation='relu'))\n",
        "model.add(Conv1D(filters=64, kernel_size=2, dilation_rate=4, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=10, batch_size=16)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "uIrTvbHMvvXk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dense, Flatten\n",
        "\n",
        "# Load your time series data\n",
        "time_series_data = pd.read_csv('time_series_data.csv')\n",
        "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
        "time_series_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Prepare data for TCN\n",
        "def create_dataset(dataset, time_step=1):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(dataset)-time_step-1):\n",
        "        a = dataset[i:(i+time_step), 0]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[i + time_step, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "time_step = 10\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
        "\n",
        "X, y = create_dataset(scaled_data, time_step)\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Define TCN model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, dilation_rate=1, activation='relu', input_shape=(time_step, 1)))\n",
        "model.add(Conv1D(filters=64, kernel_size=2, dilation_rate=2, activation='relu'))\n",
        "model.add(Conv1D(filters=64, kernel_size=2, dilation_rate=4, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=10, batch_size=16)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "zSAoarmyv1Bt"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}