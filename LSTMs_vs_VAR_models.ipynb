{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMs vs VAR models.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOloAZ7Dh2+ay248Sv89Gx0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@ignaciozamanilloherreros/lstms-vs-var-models-5af891325647)"
      ],
      "metadata": {
        "id": "22vQeBjFjH0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VAR models (vector autoregressive models) are used for multivariate time series."
      ],
      "metadata": {
        "id": "P1tr9QlgMmds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first estimate the Encoder-Decoder Model (this will be our LTSM structure) and then the VAR model."
      ],
      "metadata": {
        "id": "-FW9hnivMwqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder-Decoder LSTM overview (Python)"
      ],
      "metadata": {
        "id": "kn9B9wFPM18p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the necessary libraries\n"
      ],
      "metadata": {
        "id": "fTN5Y0x4OHcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from numpy import hstack,array\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import TimeDistributed, Dropout,LSTM,Dense,RepeatVector\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Qnj9E5kmOIMH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform our series into samples:"
      ],
      "metadata": {
        "id": "grDs_UakNPpA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dWYfQzRDiYmz"
      },
      "outputs": [],
      "source": [
        "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequences)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps_in\n",
        "\t\tout_end_ix = end_ix + n_steps_out\n",
        "\t\t# check if we are beyond the dataset\n",
        "\t\tif out_end_ix > len(sequences):\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string):\n",
        "  plt.figure(figsize=(11,5))\n",
        "\n",
        "  plt.plot(history.history[string][50:])\n",
        "  plt.plot(history.history['val_'+string][50:])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "W8cnZmzrOFL4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XS, yS = split_sequences(trainS_df, n_steps_in, n_steps_out)\n",
        "# XS_test,yS_test =split_sequences(testS_df, n_steps_in, n_steps_out)"
      ],
      "metadata": {
        "id": "6jF0BNxHjLLy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and plot our data"
      ],
      "metadata": {
        "id": "lX20tsV0ONFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df1 = pd.read_csv('SPAIN/Data_for_Python.csv', header=0, infer_datetime_format=True, parse_dates=['DATE'], index_col=['DATE'])\n",
        "\n",
        "# fig = make_subplots(rows=2, cols=2,subplot_titles=(\"Unemployment (%)\", \"GDP (10^) \", \"i rate(%)\", \"Debt\"))\n",
        "                    \n",
        "# # Top left\n",
        "# fig.add_trace(\n",
        "#     go.Scatter(x=df1.index, y=df1[\"Unem\"], name=\"Unemployment\"),\n",
        "#     row=1, col=1, secondary_y=False)\n",
        "\n",
        "# # Top right\n",
        "# fig.add_trace(\n",
        "#     go.Scatter(x=df1.index, y=df1[\"GDP\"], name=\"GDP\"),\n",
        "#     row=1, col=2, secondary_y=False,\n",
        "# )\n",
        "\n",
        "# # Bottom left\n",
        "# fig.add_trace(\n",
        "#     go.Scatter(x=df1.index, y=df1[\"i_rate\"], name=\"i rate\"),\n",
        "#     row=2, col=1, secondary_y=False,\n",
        "# )\n",
        "\n",
        "# # Bottom right\n",
        "# fig.add_trace(\n",
        "#     go.Scatter(x=df1.index, y=df1[\"Debt\"], name=\"Debt\"),\n",
        "#     row=2, col=2, secondary_y=False,\n",
        "# )\n",
        "# fig.update_layout(height=600, width=800, title_text=\"Quarterly Data: Spain\")\n",
        "\n",
        "\n",
        "\n",
        "# fig.show()"
      ],
      "metadata": {
        "id": "znmC-wcLOO4g"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the number of steps in & out\n"
      ],
      "metadata": {
        "id": "hOK08xNQOTEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps_in, n_steps_out = 10, 8"
      ],
      "metadata": {
        "id": "sAdxEchmOTxc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the samples"
      ],
      "metadata": {
        "id": "a3dq48iTOVaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trainS_df,testS_df = df1[0:72], df1[(72-n_steps_in):]\n",
        "\n",
        "# trainS_df=pd.DataFrame(trainS_df[2:]).to_numpy()\n",
        "# testS_df=pd.DataFrame(testS_df[2:]).to_numpy()\n",
        "\n",
        "# XS, yS = split_sequences(trainS_df, n_steps_in, n_steps_out)\n",
        "# XS_test,yS_test = split_sequences(testS_df, n_steps_in, n_steps_out)"
      ],
      "metadata": {
        "id": "PgYTdCPvOWqt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data of the other countries\n"
      ],
      "metadata": {
        "id": "J9eQHfuVOY6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # FRANCE\n",
        "# df2 = pd.read_csv('France/Data_for_Python.csv', header=0, infer_datetime_format=True, parse_dates=['DATE'], index_col=['DATE'])\n",
        "# trainF_df = df2[0:72]\n",
        "# trainF_df=trainF_df.to_numpy()\n",
        "# XF, yF = split_sequences(trainF_df, n_steps_in, n_steps_out)\n",
        "\n",
        "# # ITALY\n",
        "# df3 = pd.read_csv('Italy/Data_for_Python.csv', header=0, infer_datetime_format=True, parse_dates=['DATE'], index_col=['DATE'])\n",
        "# trainIT_df = df3[0:72]\n",
        "# trainIT_df=trainIT_df.to_numpy()\n",
        "# XIT, yIT = split_sequences(trainIT_df, n_steps_in, n_steps_out)\n",
        "\n",
        "# # GREECE\n",
        "# df4 = pd.read_csv('Greece/Data_for_Python.csv', header=0, infer_datetime_format=True, parse_dates=['DATE'], index_col=['DATE'])\n",
        "# trainGR_df = df4[0:68]\n",
        "# trainGR_df=trainGR_df.to_numpy()\n",
        "# XGR, yGR = split_sequences(trainGR_df, n_steps_in, n_steps_out)\n",
        "\n",
        "# # IRELAND\n",
        "# df5 = pd.read_csv('Ireland/Data_for_Python.csv', header=0, infer_datetime_format=True, parse_dates=['DATE'], index_col=['DATE'])\n",
        "# trainIR_df = df5[0:68]\n",
        "# trainIR_df=trainIR_df.to_numpy()\n",
        "# XIR, yIR = split_sequences(trainIR_df, n_steps_in, n_steps_out)\n",
        "\n",
        "# # PORTUGAL\n",
        "# df6 = pd.read_csv('Portugal/Data_for_Python.csv', header=0, infer_datetime_format=True, parse_dates=['DATE'], index_col=['DATE'])\n",
        "# trainPT_df = df6[0:68]\n",
        "# trainPT_df=trainPT_df.to_numpy()\n",
        "# XPT, yPT = split_sequences(trainPT_df, n_steps_in, n_steps_out)"
      ],
      "metadata": {
        "id": "OtBzrtEWObTH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scale the data:"
      ],
      "metadata": {
        "id": "iS4wgcOCNRe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# O=np.concatenate((trainS_df,trainF_df,trainIT_df,trainGR_df,trainIR_df,trainPT_df))\n",
        "# scaler = MinMaxScaler()\n",
        "# scaler.fit(O)\n",
        "\n",
        "# #we see an example: FRANCE\n",
        "# for i in range(len(XF)):\n",
        "#    XF[i]=scaler.transform(XF[i])\n",
        "# yF=yF[:,:,2].reshape(len(yF),n_steps_out,1)\n",
        "\n",
        "#ITALY\n",
        "# for i in range(len(XIT)):\n",
        "#    XIT[i]=scaler.transform(XIT[i])\n",
        "# yIT=yIT[:,:,2].reshape(len(yIT),n_steps_out,1)\n",
        "\n",
        "# #GREECE\n",
        "# for i in range(len(XGR)):\n",
        "#    XGR[i]=scaler.transform(XGR[i])\n",
        "# yGR=yGR[:,:,2].reshape(len(yGR),n_steps_out,1)\n",
        "\n",
        "# #IRELAND\n",
        "# for i in range(len(XIR)):\n",
        "#    XIR[i]=scaler.transform(XIR[i])\n",
        "# yIR=yIR[:,:,2].reshape(len(yIR),n_steps_out,1)\n",
        "\n",
        "# #PORTUGAL\n",
        "# for i in range(len(XPT)):\n",
        "#    XPT[i]=scaler.transform(XPT[i])\n",
        "# yPT=yPT[:,:,2].reshape(len(yPT),n_steps_out,1)\n",
        "\n",
        "# \"\"\"   SPAIN: TRAIN & TEST     \"\"\"\n",
        "# for i in range(len(XS)):\n",
        "#    XS[i]=scaler.transform(XS[i])\n",
        "\n",
        "# for i in range(len(XS_test)):\n",
        "#    XS_test[i]=scaler.transform(XS_test[i])\n",
        "   \n",
        "# yS=yS[:,:,2].reshape(len(yS),n_steps_out,1)\n",
        "# yS_test=yS_test[:,:,2].reshape(len(yS_test),n_steps_out,1)"
      ],
      "metadata": {
        "id": "lC3203TqjMfZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X=np.concatenate((XS,XF,XIT,XGR,XIR,XPT))\n",
        "# y=np.concatenate((yS,yF,yIT,yGR,yIR,yPT))"
      ],
      "metadata": {
        "id": "3dOCMnHajO7p"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model creation & hyperparameter tuning:\n"
      ],
      "metadata": {
        "id": "Wd7jpI5uNWyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.random.set_seed(1265)\n",
        "\n",
        "# o1=tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
        "\n",
        "# n_features = X.shape[2]\n",
        "# # define model\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(12, activation='relu', input_shape=(n_steps_in, n_features)))\n",
        "# model.add(RepeatVector(n_steps_out))\n",
        "# model.add(LSTM(12, activation='relu', return_sequences=True))\n",
        "# model.add(Dropout(0.45))\n",
        "# model.add(TimeDistributed(Dense(1)))\n",
        "# model.compile(optimizer=o1, loss='mse', metrics=[\"mae\"])\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# # fit model\n",
        "# history=model.fit(X, y, epochs=3000, verbose=0,validation_data=(XS_test,yS_test),batch_size=len(X))"
      ],
      "metadata": {
        "id": "qAuB1TQqjR1e"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "id": "3f6J57-pOpZB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "LrZ_nIatNiK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# U=np.zeros((8, 17))\n",
        "# Rs= [None] * 17\n",
        "\n",
        "# j=0\n",
        "# aux1=df1.diff()\n",
        "# aux2=aux1.diff()\n",
        "# aux3 = scaler.transform(aux2)\n",
        "# aux4=scaler.transform(df1)\n",
        "# i=88\n",
        "# for i in range(72,89):\n",
        "#     aux5=df1[0:i]  \n",
        "#     datos_entrada=aux4[(i-n_steps_in):(i)]\n",
        "#     datos_entrada=pd.DataFrame(datos_entrada).to_numpy()\n",
        "#     datos_entrada2=datos_entrada.reshape(1,n_steps_in,n_features)\n",
        "#     yhat = model.predict(datos_entrada2, verbose=0)\n",
        "#     yhat=yhat.reshape(n_steps_out)\n",
        "#     Yp=yhat\n",
        "#     Yp2=np.concatenate((aux5[\"Unem\"],Yp))\n",
        "#     Rs[j]= Yp2\n",
        "#     Rv=df1[\"Unem\"][0:(i+n_steps_out)].to_numpy()\n",
        "#     l=np.zeros((8))    \n",
        "#     l2=Rv[(i):(i+n_steps_out)]\n",
        "#     l[0:len(l2)]=l2\n",
        "#     a2= Yp2[(i):(i+n_steps_out)]\n",
        "#     a3=np.zeros(8) \n",
        "#     a3[0:len(l2)]=a2[0:len(l2)]\n",
        "#     R=l-a3\n",
        "#     U[:,j]=R\n",
        "#     j=j+1\n",
        "    \n",
        "# U=abs(U)"
      ],
      "metadata": {
        "id": "2n7YdFlajSKv"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAR model overview (in R)"
      ],
      "metadata": {
        "id": "jxBTIO1vNksf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![VAR](https://miro.medium.com/max/1056/1*pIHYGdT4AmMmHT-61UTUwQ.png)"
      ],
      "metadata": {
        "id": "8f3fJUHMNnTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "UgRW8rBgNxTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y1<-diff(gdp$GDP[1:72])\n",
        "# plot(y1,type=\"l\")\n",
        "# adf.test(y1)\n",
        "# y2<-diff(y1)\n",
        "# plot(y2,type=\"l\")\n",
        "# adf.test(y2)"
      ],
      "metadata": {
        "id": "nL-ZlO7xNkfy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model creation"
      ],
      "metadata": {
        "id": "_OFcOTZtNwLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# library(tseries)\n",
        "# library(forcats)\n",
        "# library(vars)"
      ],
      "metadata": {
        "id": "4mPXCRmwNu7q"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Model creation\n",
        "# y<-matrix(c(y2,e2,u2,in2),c(length(y2),4))\n",
        "# colnames(y) <- cbind(\"GDP\",\"Debt\",\"Unem\", \"i_rate\")"
      ],
      "metadata": {
        "id": "nmIO8h-lN0r3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lagselect <- VARselect(y, lag.max = 16, type = \"const\")\n",
        "# lagselect$selection\n",
        "# Model1 <- VAR(y, p = 10, type = \"const\", season = NULL, exog = NULL) \n",
        "# summary(Model1)\n",
        "# view raw"
      ],
      "metadata": {
        "id": "95O4VzRyN0-a"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Granger causality test"
      ],
      "metadata": {
        "id": "eAgzxJKfN4K8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Granger causality test\n",
        "# GrangerGDP<- causality(Model1, cause = \"GDP\")\n",
        "# GrangerGDP\n",
        "# Grangeri_rate<- causality(Model1, cause = \"i_rate\")\n",
        "# Grangeri_rate\n",
        "# GrangerUnem<- causality(Model1, cause = \"Unem\")\n",
        "# GrangerUnem\n",
        "# GrangerDebt<- causality(Model1, cause = \"Debt\")\n",
        "# GrangerDebt"
      ],
      "metadata": {
        "id": "TZ8d23EeN3Gj"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "v3s_zGTlN7AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WE MAKE ALL THE PERIODS PREDICTIONS\n",
        "# j=0\n",
        "# U=matrix(0, nrow = 8, ncol = 17)\n",
        "# for(i in seq(72, 88, 1)){\n",
        "# j=j+1\n",
        "#  y1<-diff(gdp$GDP[1:i])\n",
        "#  y2<-diff(y1)\n",
        "  \n",
        "#  e1<-diff(Debt$DEBT[1:i])\n",
        "#  e2<-diff(e1)\n",
        "# u1<-diff(Unem$Unem[1:i])\n",
        "#  u2<-diff(u1)\n",
        "# in1<-diff(Ecb$i_rate[1:i])\n",
        "#  in2<-diff(in1)\n",
        "# #Model creation\n",
        "# y<-matrix(c(y2,e2,u2,in2),c(length(y2),4))\n",
        "#  colnames(y) <- cbind(\"GDP\",\"Debt\",\"Unem\", \"i_rate\")\n",
        "# lagselect <- VARselect(y, lag.max = 16, type = \"const\")\n",
        "#  lagselect$selection\n",
        "#  Model1 <- VAR(y, p = 10, type = \"const\", season = NULL, exog = NULL)\n",
        "# steps=8\n",
        "#  forecast <- predict(Model1, n.ahead = steps, ci = 0.95)\n",
        "# gdp_forecast=forecast$fcst$GDP[,1]\n",
        "#  debt_forecast=forecast$fcst$Debt[,1]\n",
        "#  unem_forecast=forecast$fcst$Unem[,1]\n",
        "#  i_rate_forecast=forecast$fcst$i_rate[,1]\n",
        "# #We focus on unemployment\n",
        "#  unem_plus_forecast<-as.vector(c(u2,unem_forecast))\n",
        "#  unem_plus_forecast_ad1<-diffinv(unem_plus_forecast, xi=u1[1])\n",
        "#  unem_plus_forecast_ad2<-diffinv(unem_plus_forecast_ad1, xi=Unem$Unem[1])\n",
        " \n",
        "#  unem_plus_forecast_ad2[(i+1):(i+steps)]#predictions\n",
        "#  Unem$Unem[(i+1):(i+steps)]#real values\n",
        " \n",
        "#  U[,j]= Unem$Unem[(i+1):(i+steps)]-unem_plus_forecast_ad2[(i+1):(i+steps)]\n",
        "#  #In U we are going to store the residuals\n",
        " \n",
        " \n",
        "# }\n",
        "# U=abs(U)\n",
        "# rowMeans(U,na.rm=TRUE)"
      ],
      "metadata": {
        "id": "djd2lCG5N523"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance comparison\n"
      ],
      "metadata": {
        "id": "fSyQEkvfOxp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the error matrix of the VAR model\n"
      ],
      "metadata": {
        "id": "LZQSTHF4OywB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# U_R=pd.read_csv(\"SPAIN/U_R.csv\")"
      ],
      "metadata": {
        "id": "mXsRFkdEOx_F"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate the mae for each timestep for both models & and the mean of this values"
      ],
      "metadata": {
        "id": "dMIVAYWMO1Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# U[U == 0] = np.nan\n",
        "# ULS=np.nanmean(U,axis=1)\n",
        "# np.mean(ULS)\n",
        "    \n",
        "#     #     VAR model \n",
        "# UR=np.nanmean(U_R,axis=1)\n",
        "# np.mean(UR)"
      ],
      "metadata": {
        "id": "Yid9TbOeO0Gf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive visualization of the performance"
      ],
      "metadata": {
        "id": "1kln8gwJO4IC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n"
      ],
      "metadata": {
        "id": "fEZ7AoyFO6JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ULS=ULS.reshape(8,1)\n",
        "# UR=UR.reshape(8,1)\n",
        "\n",
        "# U1=pd.DataFrame(ULS).set_axis([\"mae\"],axis=1)\n",
        "# U1[\"Model\"]=\"LSTM\"\n",
        "# U1[\"N step ahead\"]=np.arange(1,9)\n",
        "\n",
        "# U2=pd.DataFrame(UR).set_axis([\"mae\"],axis=1)\n",
        "# U2[\"Model\"]=\"VAR\"\n",
        "# U2[\"N step ahead\"]=np.arange(1,9)\n",
        "\n",
        "# U3=pd.concat([U1,U2])"
      ],
      "metadata": {
        "id": "lxurEsEwO3Nu"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data visualization\n"
      ],
      "metadata": {
        "id": "uqBgdLUWO7p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fig = px.scatter(U3, x=\"N step ahead\", y=\"mae\", color=\"Model\",width=900, height=500,color_discrete_sequence=[\"red\", \"blue\"])\n",
        "# fig.update_traces(marker=dict(size=30,\n",
        "#                               line=dict(width=2,\n",
        "#                                         color='DarkSlateGrey')),\n",
        "#                   selector=dict(mode='markers'))\n",
        "# fig.update_layout(\n",
        "#      title=dict(\n",
        "#         text='<b>Model</b>',\n",
        "#         x=0.45,\n",
        "#         y=0.97,\n",
        "#         font=dict(\n",
        "#             family=\"Arial\",\n",
        "#             size=25,\n",
        "#             color='#000000'\n",
        "#         )\n",
        "#     ),\n",
        "\n",
        "# )\n",
        "\n",
        "\n",
        "# fig.show()"
      ],
      "metadata": {
        "id": "MzzEiN5SO6z6"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}